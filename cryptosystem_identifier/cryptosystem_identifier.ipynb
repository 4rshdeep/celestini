{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Username for 'https://github.com': "
     ]
    }
   ],
   "source": [
    "!git pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "from features import calculateIC, frequency_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DIR = \"./data_gen/data/train/\"\n",
    "TEST_DIR = \"./data_gen/data/test/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "plain = open(TRAIN_DIR+ \"plain.txt\").read().splitlines()\n",
    "ss = open(TRAIN_DIR + \"simple_sub.txt\").read().splitlines()\n",
    "vig = open(TRAIN_DIR + \"vig.txt\").read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_plain = open(TEST_DIR+ \"plain.txt\").read().splitlines()\n",
    "test_ss = open(TEST_DIR + \"simple_sub.txt\").read().splitlines()\n",
    "test_vig = open(TEST_DIR + \"vig.txt\").read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ic_list(l):\n",
    "    ic = []\n",
    "    for i in l:\n",
    "        ic_i = calculateIC(i)\n",
    "        ic += [ic_i]\n",
    "    return ic\n",
    "\n",
    "def get_freq_list(l):\n",
    "    freq = []\n",
    "    for i in l:\n",
    "        freq_i = frequency_index(i)\n",
    "        freq += [freq_i]\n",
    "    return freq\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_ss = get_freq_list(ss)\n",
    "ic_ss = get_ic_list(ss)\n",
    "freq_vig = get_freq_list(vig)\n",
    "ic_vig = get_ic_list(vig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_freq_ss = get_freq_list(test_ss)\n",
    "test_ic_ss = get_ic_list(test_ss)\n",
    "test_freq_vig = get_freq_list(test_vig)\n",
    "test_ic_vig = get_ic_list(test_vig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([-0.3743199999999999,\n",
       "  -0.37432,\n",
       "  -0.4439799999999999,\n",
       "  -0.5114699999999998,\n",
       "  -0.4439799999999999],\n",
       " [8.326672684688674e-17,\n",
       "  1.6653345369377348e-16,\n",
       "  -0.1270199999999999,\n",
       "  2.220446049250313e-16,\n",
       "  -0.21758000000000005],\n",
       " [0.06247194373784229,\n",
       "  0.06875654646117013,\n",
       "  0.07290372670807453,\n",
       "  0.07553101756000306,\n",
       "  0.0714622641509434],\n",
       " [0.04361813556785875,\n",
       "  0.04309441867424809,\n",
       "  0.04759316770186335,\n",
       "  0.044245073230580474,\n",
       "  0.044182389937106915])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_ss[:5], freq_vig[:5], ic_ss[:5], ic_vig[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "plt.plot(ic_ss, freq_ss, 'bo', alpha=0.8)\n",
    "plt.plot(ic_vig, freq_vig, 'r^', alpha=0.8)\n",
    "plt.xlabel(\"IC\")\n",
    "plt.ylabel(\"FREQUENCY\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vig = np.array([ic_vig, freq_vig]).T\n",
    "train_ss = np.array([ic_ss, freq_ss]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vig = np.array([test_ic_vig, test_freq_vig]).T\n",
    "test_ss = np.array([test_ic_ss, test_freq_ss]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.04361813556785875,\n",
       "  0.04309441867424809,\n",
       "  0.04759316770186335,\n",
       "  0.044245073230580474,\n",
       "  0.044182389937106915],\n",
       " array([0.04361814, 0.04309442, 0.04759317, 0.04424507, 0.04418239]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ic_vig[:5], train_vig[:5,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAcsAAAG3CAYAAADb4n79AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEEBYAgAQQFgCABBAWAIAEEBYAgAQ8P8BSN5BhK0khVgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1865fcf278>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(7,7))\n",
    "plt.plot(train_ss[:,0], train_ss[:,1], 'bo')\n",
    "plt.plot(train_vig[:,0], train_vig[:,1], 'ro')\n",
    "plt.xlabel(\"IC\")\n",
    "plt.ylabel(\"FREQUENCY\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zero for simple sub and one for vigenere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "yv = np.ones(50)\n",
    "ys = np.zeros(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50,), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yv.shape, ys[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.vstack((train_ss, train_vig))\n",
    "y_train = np.hstack((ys, yv)).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = np.vstack((test_ss, test_vig))\n",
    "y_test = np.hstack((np.zeros(50), np.ones(50)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, [None, 2])\n",
    "W = tf.get_variable(\"W\", shape=[2, 1],\n",
    "           initializer=tf.contrib.layers.xavier_initializer())\n",
    "b = tf.Variable(tf.zeros([1]))\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = tf.nn.softmax(tf.matmul(X, W) + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_ = tf.placeholder(tf.float32, [None, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = -tf.reduce_sum(Y_ * tf.log(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_correct = tf.equal(tf.arg_max(Y_, 1), tf.arg_max(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer()\n",
    "train_step = optimizer.minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 2)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 1)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "FailedPreconditionError",
     "evalue": "Attempting to use uninitialized value beta1_power_2\n\t [[Node: beta1_power_2/read = Identity[T=DT_FLOAT, _class=[\"loc:@Variable\"], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](beta1_power_2)]]\n\t [[Node: Softmax_2/_7 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_120_Softmax_2\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n\nCaused by op 'beta1_power_2/read', defined at:\n  File \"/usr/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python3.5/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2850, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-56-5c2ae8978455>\", line 2, in <module>\n    train_step = optimizer.minimize(cross_entropy)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/optimizer.py\", line 369, in minimize\n    name=name)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/optimizer.py\", line 520, in apply_gradients\n    self._create_slots([_get_variable_for(v) for v in var_list])\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/adam.py\", line 124, in _create_slots\n    colocate_with=first_var)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/optimizer.py\", line 629, in _create_non_slot_variable\n    v = variable_scope.variable(initial_value, name=name, trainable=False)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variable_scope.py\", line 2157, in variable\n    use_resource=use_resource)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variable_scope.py\", line 2147, in <lambda>\n    previous_getter = lambda **kwargs: default_variable_creator(None, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variable_scope.py\", line 2130, in default_variable_creator\n    constraint=constraint)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variables.py\", line 233, in __init__\n    constraint=constraint)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variables.py\", line 381, in _init_from_args\n    self._snapshot = array_ops.identity(self._variable, name=\"read\")\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/array_ops.py\", line 131, in identity\n    return gen_array_ops.identity(input, name=name)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_array_ops.py\", line 2051, in identity\n    \"Identity\", input=input, name=name)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 3271, in create_op\n    op_def=op_def)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 1650, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nFailedPreconditionError (see above for traceback): Attempting to use uninitialized value beta1_power_2\n\t [[Node: beta1_power_2/read = Identity[T=DT_FLOAT, _class=[\"loc:@Variable\"], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](beta1_power_2)]]\n\t [[Node: Softmax_2/_7 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_120_Softmax_2\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1360\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1361\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1362\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1339\u001b[0m           return tf_session.TF_Run(session, options, feed_dict, fetch_list,\n\u001b[0;32m-> 1340\u001b[0;31m                                    target_list, status, run_metadata)\n\u001b[0m\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    515\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 516\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    517\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFailedPreconditionError\u001b[0m: Attempting to use uninitialized value beta1_power_2\n\t [[Node: beta1_power_2/read = Identity[T=DT_FLOAT, _class=[\"loc:@Variable\"], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](beta1_power_2)]]\n\t [[Node: Softmax_2/_7 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_120_Softmax_2\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-13fed92b3474>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcross_entropy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss after {} -- {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1135\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1136\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1137\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1138\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1353\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1354\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1355\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1356\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1357\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1372\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1373\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1374\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1376\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFailedPreconditionError\u001b[0m: Attempting to use uninitialized value beta1_power_2\n\t [[Node: beta1_power_2/read = Identity[T=DT_FLOAT, _class=[\"loc:@Variable\"], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](beta1_power_2)]]\n\t [[Node: Softmax_2/_7 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_120_Softmax_2\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n\nCaused by op 'beta1_power_2/read', defined at:\n  File \"/usr/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python3.5/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2850, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-56-5c2ae8978455>\", line 2, in <module>\n    train_step = optimizer.minimize(cross_entropy)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/optimizer.py\", line 369, in minimize\n    name=name)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/optimizer.py\", line 520, in apply_gradients\n    self._create_slots([_get_variable_for(v) for v in var_list])\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/adam.py\", line 124, in _create_slots\n    colocate_with=first_var)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/optimizer.py\", line 629, in _create_non_slot_variable\n    v = variable_scope.variable(initial_value, name=name, trainable=False)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variable_scope.py\", line 2157, in variable\n    use_resource=use_resource)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variable_scope.py\", line 2147, in <lambda>\n    previous_getter = lambda **kwargs: default_variable_creator(None, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variable_scope.py\", line 2130, in default_variable_creator\n    constraint=constraint)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variables.py\", line 233, in __init__\n    constraint=constraint)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variables.py\", line 381, in _init_from_args\n    self._snapshot = array_ops.identity(self._variable, name=\"read\")\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/array_ops.py\", line 131, in identity\n    return gen_array_ops.identity(input, name=name)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_array_ops.py\", line 2051, in identity\n    \"Identity\", input=input, name=name)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 3271, in create_op\n    op_def=op_def)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 1650, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nFailedPreconditionError (see above for traceback): Attempting to use uninitialized value beta1_power_2\n\t [[Node: beta1_power_2/read = Identity[T=DT_FLOAT, _class=[\"loc:@Variable\"], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](beta1_power_2)]]\n\t [[Node: Softmax_2/_7 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_120_Softmax_2\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n"
     ]
    }
   ],
   "source": [
    "for step in range(200):\n",
    "    _, l, pred = sess.run([train_step, cross_entropy, Y], feed_dict={X : x_train, Y_ : y_train})\n",
    "    \n",
    "    if(step%5==0):\n",
    "        print('Loss after {} -- {}'.format(step, l))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(1, input_shape=(2,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "0s - loss: 0.0735 - acc: 0.6700\n",
      "Epoch 2/500\n",
      "0s - loss: 0.0734 - acc: 0.6700\n",
      "Epoch 3/500\n",
      "0s - loss: 0.0734 - acc: 0.6700\n",
      "Epoch 4/500\n",
      "0s - loss: 0.0734 - acc: 0.6700\n",
      "Epoch 5/500\n",
      "0s - loss: 0.0733 - acc: 0.6700\n",
      "Epoch 6/500\n",
      "0s - loss: 0.0733 - acc: 0.6700\n",
      "Epoch 7/500\n",
      "0s - loss: 0.0733 - acc: 0.6700\n",
      "Epoch 8/500\n",
      "0s - loss: 0.0732 - acc: 0.6700\n",
      "Epoch 9/500\n",
      "0s - loss: 0.0733 - acc: 0.6700\n",
      "Epoch 10/500\n",
      "0s - loss: 0.0732 - acc: 0.6700\n",
      "Epoch 11/500\n",
      "0s - loss: 0.0732 - acc: 0.6700\n",
      "Epoch 12/500\n",
      "0s - loss: 0.0732 - acc: 0.6700\n",
      "Epoch 13/500\n",
      "0s - loss: 0.0732 - acc: 0.6700\n",
      "Epoch 14/500\n",
      "0s - loss: 0.0731 - acc: 0.6700\n",
      "Epoch 15/500\n",
      "0s - loss: 0.0733 - acc: 0.6700\n",
      "Epoch 16/500\n",
      "0s - loss: 0.0732 - acc: 0.6700\n",
      "Epoch 17/500\n",
      "0s - loss: 0.0731 - acc: 0.6700\n",
      "Epoch 18/500\n",
      "0s - loss: 0.0730 - acc: 0.6700\n",
      "Epoch 19/500\n",
      "0s - loss: 0.0730 - acc: 0.6700\n",
      "Epoch 20/500\n",
      "0s - loss: 0.0730 - acc: 0.6700\n",
      "Epoch 21/500\n",
      "0s - loss: 0.0730 - acc: 0.6700\n",
      "Epoch 22/500\n",
      "0s - loss: 0.0730 - acc: 0.6700\n",
      "Epoch 23/500\n",
      "0s - loss: 0.0730 - acc: 0.6700\n",
      "Epoch 24/500\n",
      "0s - loss: 0.0729 - acc: 0.6700\n",
      "Epoch 25/500\n",
      "0s - loss: 0.0729 - acc: 0.6700\n",
      "Epoch 26/500\n",
      "0s - loss: 0.0728 - acc: 0.6700\n",
      "Epoch 27/500\n",
      "0s - loss: 0.0728 - acc: 0.6700\n",
      "Epoch 28/500\n",
      "0s - loss: 0.0729 - acc: 0.6700\n",
      "Epoch 29/500\n",
      "0s - loss: 0.0729 - acc: 0.6700\n",
      "Epoch 30/500\n",
      "0s - loss: 0.0728 - acc: 0.6700\n",
      "Epoch 31/500\n",
      "0s - loss: 0.0730 - acc: 0.6700\n",
      "Epoch 32/500\n",
      "0s - loss: 0.0728 - acc: 0.6700\n",
      "Epoch 33/500\n",
      "0s - loss: 0.0727 - acc: 0.6700\n",
      "Epoch 34/500\n",
      "0s - loss: 0.0727 - acc: 0.6700\n",
      "Epoch 35/500\n",
      "0s - loss: 0.0727 - acc: 0.6700\n",
      "Epoch 36/500\n",
      "0s - loss: 0.0726 - acc: 0.6700\n",
      "Epoch 37/500\n",
      "0s - loss: 0.0726 - acc: 0.6700\n",
      "Epoch 38/500\n",
      "0s - loss: 0.0727 - acc: 0.6700\n",
      "Epoch 39/500\n",
      "0s - loss: 0.0726 - acc: 0.6700\n",
      "Epoch 40/500\n",
      "0s - loss: 0.0726 - acc: 0.6700\n",
      "Epoch 41/500\n",
      "0s - loss: 0.0725 - acc: 0.6700\n",
      "Epoch 42/500\n",
      "0s - loss: 0.0725 - acc: 0.6700\n",
      "Epoch 43/500\n",
      "0s - loss: 0.0725 - acc: 0.6700\n",
      "Epoch 44/500\n",
      "0s - loss: 0.0725 - acc: 0.6700\n",
      "Epoch 45/500\n",
      "0s - loss: 0.0725 - acc: 0.6700\n",
      "Epoch 46/500\n",
      "0s - loss: 0.0724 - acc: 0.6700\n",
      "Epoch 47/500\n",
      "0s - loss: 0.0724 - acc: 0.6700\n",
      "Epoch 48/500\n",
      "0s - loss: 0.0723 - acc: 0.6700\n",
      "Epoch 49/500\n",
      "0s - loss: 0.0725 - acc: 0.6700\n",
      "Epoch 50/500\n",
      "0s - loss: 0.0724 - acc: 0.6700\n",
      "Epoch 51/500\n",
      "0s - loss: 0.0723 - acc: 0.6700\n",
      "Epoch 52/500\n",
      "0s - loss: 0.0723 - acc: 0.6700\n",
      "Epoch 53/500\n",
      "0s - loss: 0.0723 - acc: 0.6700\n",
      "Epoch 54/500\n",
      "0s - loss: 0.0722 - acc: 0.6700\n",
      "Epoch 55/500\n",
      "0s - loss: 0.0722 - acc: 0.6700\n",
      "Epoch 56/500\n",
      "0s - loss: 0.0722 - acc: 0.6700\n",
      "Epoch 57/500\n",
      "0s - loss: 0.0722 - acc: 0.6700\n",
      "Epoch 58/500\n",
      "0s - loss: 0.0721 - acc: 0.6700\n",
      "Epoch 59/500\n",
      "0s - loss: 0.0722 - acc: 0.6700\n",
      "Epoch 60/500\n",
      "0s - loss: 0.0721 - acc: 0.6700\n",
      "Epoch 61/500\n",
      "0s - loss: 0.0721 - acc: 0.6700\n",
      "Epoch 62/500\n",
      "0s - loss: 0.0721 - acc: 0.6700\n",
      "Epoch 63/500\n",
      "0s - loss: 0.0720 - acc: 0.6700\n",
      "Epoch 64/500\n",
      "0s - loss: 0.0721 - acc: 0.6700\n",
      "Epoch 65/500\n",
      "0s - loss: 0.0720 - acc: 0.6700\n",
      "Epoch 66/500\n",
      "0s - loss: 0.0720 - acc: 0.6700\n",
      "Epoch 67/500\n",
      "0s - loss: 0.0720 - acc: 0.6700\n",
      "Epoch 68/500\n",
      "0s - loss: 0.0719 - acc: 0.6700\n",
      "Epoch 69/500\n",
      "0s - loss: 0.0719 - acc: 0.6700\n",
      "Epoch 70/500\n",
      "0s - loss: 0.0719 - acc: 0.6700\n",
      "Epoch 71/500\n",
      "0s - loss: 0.0719 - acc: 0.6700\n",
      "Epoch 72/500\n",
      "0s - loss: 0.0719 - acc: 0.6700\n",
      "Epoch 73/500\n",
      "0s - loss: 0.0719 - acc: 0.6700\n",
      "Epoch 74/500\n",
      "0s - loss: 0.0719 - acc: 0.6700\n",
      "Epoch 75/500\n",
      "0s - loss: 0.0718 - acc: 0.6700\n",
      "Epoch 76/500\n",
      "0s - loss: 0.0718 - acc: 0.6700\n",
      "Epoch 77/500\n",
      "0s - loss: 0.0718 - acc: 0.6700\n",
      "Epoch 78/500\n",
      "0s - loss: 0.0719 - acc: 0.6700\n",
      "Epoch 79/500\n",
      "0s - loss: 0.0717 - acc: 0.6700\n",
      "Epoch 80/500\n",
      "0s - loss: 0.0718 - acc: 0.6700\n",
      "Epoch 81/500\n",
      "0s - loss: 0.0717 - acc: 0.6700\n",
      "Epoch 82/500\n",
      "0s - loss: 0.0717 - acc: 0.6700\n",
      "Epoch 83/500\n",
      "0s - loss: 0.0716 - acc: 0.6700\n",
      "Epoch 84/500\n",
      "0s - loss: 0.0717 - acc: 0.6700\n",
      "Epoch 85/500\n",
      "0s - loss: 0.0716 - acc: 0.6700\n",
      "Epoch 86/500\n",
      "0s - loss: 0.0716 - acc: 0.6700\n",
      "Epoch 87/500\n",
      "0s - loss: 0.0716 - acc: 0.6700\n",
      "Epoch 88/500\n",
      "0s - loss: 0.0716 - acc: 0.6700\n",
      "Epoch 89/500\n",
      "0s - loss: 0.0715 - acc: 0.6700\n",
      "Epoch 90/500\n",
      "0s - loss: 0.0716 - acc: 0.6700\n",
      "Epoch 91/500\n",
      "0s - loss: 0.0715 - acc: 0.6700\n",
      "Epoch 92/500\n",
      "0s - loss: 0.0714 - acc: 0.6700\n",
      "Epoch 93/500\n",
      "0s - loss: 0.0714 - acc: 0.6700\n",
      "Epoch 94/500\n",
      "0s - loss: 0.0715 - acc: 0.6700\n",
      "Epoch 95/500\n",
      "0s - loss: 0.0714 - acc: 0.6700\n",
      "Epoch 96/500\n",
      "0s - loss: 0.0714 - acc: 0.6700\n",
      "Epoch 97/500\n",
      "0s - loss: 0.0714 - acc: 0.6700\n",
      "Epoch 98/500\n",
      "0s - loss: 0.0714 - acc: 0.6700\n",
      "Epoch 99/500\n",
      "0s - loss: 0.0713 - acc: 0.6700\n",
      "Epoch 100/500\n",
      "0s - loss: 0.0713 - acc: 0.6700\n",
      "Epoch 101/500\n",
      "0s - loss: 0.0713 - acc: 0.6700\n",
      "Epoch 102/500\n",
      "0s - loss: 0.0712 - acc: 0.6700\n",
      "Epoch 103/500\n",
      "0s - loss: 0.0713 - acc: 0.6700\n",
      "Epoch 104/500\n",
      "0s - loss: 0.0712 - acc: 0.6700\n",
      "Epoch 105/500\n",
      "0s - loss: 0.0712 - acc: 0.6700\n",
      "Epoch 106/500\n",
      "0s - loss: 0.0712 - acc: 0.6700\n",
      "Epoch 107/500\n",
      "0s - loss: 0.0711 - acc: 0.6700\n",
      "Epoch 108/500\n",
      "0s - loss: 0.0711 - acc: 0.6700\n",
      "Epoch 109/500\n",
      "0s - loss: 0.0711 - acc: 0.6700\n",
      "Epoch 110/500\n",
      "0s - loss: 0.0711 - acc: 0.6700\n",
      "Epoch 111/500\n",
      "0s - loss: 0.0711 - acc: 0.6700\n",
      "Epoch 112/500\n",
      "0s - loss: 0.0710 - acc: 0.6700\n",
      "Epoch 113/500\n",
      "0s - loss: 0.0712 - acc: 0.6700\n",
      "Epoch 114/500\n",
      "0s - loss: 0.0711 - acc: 0.6700\n",
      "Epoch 115/500\n",
      "0s - loss: 0.0711 - acc: 0.6700\n",
      "Epoch 116/500\n",
      "0s - loss: 0.0710 - acc: 0.6700\n",
      "Epoch 117/500\n",
      "0s - loss: 0.0709 - acc: 0.6700\n",
      "Epoch 118/500\n",
      "0s - loss: 0.0709 - acc: 0.6700\n",
      "Epoch 119/500\n",
      "0s - loss: 0.0709 - acc: 0.6700\n",
      "Epoch 120/500\n",
      "0s - loss: 0.0709 - acc: 0.6700\n",
      "Epoch 121/500\n",
      "0s - loss: 0.0709 - acc: 0.6700\n",
      "Epoch 122/500\n",
      "0s - loss: 0.0709 - acc: 0.6700\n",
      "Epoch 123/500\n",
      "0s - loss: 0.0709 - acc: 0.6700\n",
      "Epoch 124/500\n",
      "0s - loss: 0.0708 - acc: 0.6700\n",
      "Epoch 125/500\n",
      "0s - loss: 0.0708 - acc: 0.6700\n",
      "Epoch 126/500\n",
      "0s - loss: 0.0708 - acc: 0.6700\n",
      "Epoch 127/500\n",
      "0s - loss: 0.0709 - acc: 0.6700\n",
      "Epoch 128/500\n",
      "0s - loss: 0.0707 - acc: 0.6700\n",
      "Epoch 129/500\n",
      "0s - loss: 0.0708 - acc: 0.6700\n",
      "Epoch 130/500\n",
      "0s - loss: 0.0707 - acc: 0.6700\n",
      "Epoch 131/500\n",
      "0s - loss: 0.0707 - acc: 0.6700\n",
      "Epoch 132/500\n",
      "0s - loss: 0.0707 - acc: 0.6700\n",
      "Epoch 133/500\n",
      "0s - loss: 0.0707 - acc: 0.6700\n",
      "Epoch 134/500\n",
      "0s - loss: 0.0706 - acc: 0.6700\n",
      "Epoch 135/500\n",
      "0s - loss: 0.0706 - acc: 0.6700\n",
      "Epoch 136/500\n",
      "0s - loss: 0.0707 - acc: 0.6700\n",
      "Epoch 137/500\n",
      "0s - loss: 0.0706 - acc: 0.6700\n",
      "Epoch 138/500\n",
      "0s - loss: 0.0705 - acc: 0.6700\n",
      "Epoch 139/500\n",
      "0s - loss: 0.0706 - acc: 0.6700\n",
      "Epoch 140/500\n",
      "0s - loss: 0.0705 - acc: 0.6700\n",
      "Epoch 141/500\n",
      "0s - loss: 0.0705 - acc: 0.6700\n",
      "Epoch 142/500\n",
      "0s - loss: 0.0706 - acc: 0.6700\n",
      "Epoch 143/500\n",
      "0s - loss: 0.0704 - acc: 0.6700\n",
      "Epoch 144/500\n",
      "0s - loss: 0.0705 - acc: 0.6700\n",
      "Epoch 145/500\n",
      "0s - loss: 0.0704 - acc: 0.6700\n",
      "Epoch 146/500\n",
      "0s - loss: 0.0705 - acc: 0.6700\n",
      "Epoch 147/500\n",
      "0s - loss: 0.0704 - acc: 0.6700\n",
      "Epoch 148/500\n",
      "0s - loss: 0.0703 - acc: 0.6700\n",
      "Epoch 149/500\n",
      "0s - loss: 0.0704 - acc: 0.6700\n",
      "Epoch 150/500\n",
      "0s - loss: 0.0703 - acc: 0.6700\n",
      "Epoch 151/500\n",
      "0s - loss: 0.0703 - acc: 0.6700\n",
      "Epoch 152/500\n",
      "0s - loss: 0.0703 - acc: 0.6700\n",
      "Epoch 153/500\n",
      "0s - loss: 0.0703 - acc: 0.6700\n",
      "Epoch 154/500\n",
      "0s - loss: 0.0703 - acc: 0.6700\n",
      "Epoch 155/500\n",
      "0s - loss: 0.0702 - acc: 0.6700\n",
      "Epoch 156/500\n",
      "0s - loss: 0.0703 - acc: 0.6700\n",
      "Epoch 157/500\n",
      "0s - loss: 0.0701 - acc: 0.6700\n",
      "Epoch 158/500\n",
      "0s - loss: 0.0702 - acc: 0.6700\n",
      "Epoch 159/500\n",
      "0s - loss: 0.0702 - acc: 0.6700\n",
      "Epoch 160/500\n",
      "0s - loss: 0.0702 - acc: 0.6700\n",
      "Epoch 161/500\n",
      "0s - loss: 0.0702 - acc: 0.6700\n",
      "Epoch 162/500\n",
      "0s - loss: 0.0701 - acc: 0.6700\n",
      "Epoch 163/500\n",
      "0s - loss: 0.0701 - acc: 0.6700\n",
      "Epoch 164/500\n",
      "0s - loss: 0.0700 - acc: 0.6700\n",
      "Epoch 165/500\n",
      "0s - loss: 0.0700 - acc: 0.6700\n",
      "Epoch 166/500\n",
      "0s - loss: 0.0700 - acc: 0.6700\n",
      "Epoch 167/500\n",
      "0s - loss: 0.0700 - acc: 0.6700\n",
      "Epoch 168/500\n",
      "0s - loss: 0.0700 - acc: 0.6700\n",
      "Epoch 169/500\n",
      "0s - loss: 0.0700 - acc: 0.6700\n",
      "Epoch 170/500\n",
      "0s - loss: 0.0700 - acc: 0.6700\n",
      "Epoch 171/500\n",
      "0s - loss: 0.0699 - acc: 0.6700\n",
      "Epoch 172/500\n",
      "0s - loss: 0.0700 - acc: 0.6700\n",
      "Epoch 173/500\n",
      "0s - loss: 0.0699 - acc: 0.6700\n",
      "Epoch 174/500\n",
      "0s - loss: 0.0699 - acc: 0.6700\n",
      "Epoch 175/500\n",
      "0s - loss: 0.0699 - acc: 0.6700\n",
      "Epoch 176/500\n",
      "0s - loss: 0.0699 - acc: 0.6700\n",
      "Epoch 177/500\n",
      "0s - loss: 0.0698 - acc: 0.6700\n",
      "Epoch 178/500\n",
      "0s - loss: 0.0698 - acc: 0.6700\n",
      "Epoch 179/500\n",
      "0s - loss: 0.0698 - acc: 0.6700\n",
      "Epoch 180/500\n",
      "0s - loss: 0.0698 - acc: 0.6700\n",
      "Epoch 181/500\n",
      "0s - loss: 0.0698 - acc: 0.6700\n",
      "Epoch 182/500\n",
      "0s - loss: 0.0698 - acc: 0.6700\n",
      "Epoch 183/500\n",
      "0s - loss: 0.0698 - acc: 0.6700\n",
      "Epoch 184/500\n",
      "0s - loss: 0.0697 - acc: 0.6700\n",
      "Epoch 185/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0s - loss: 0.0697 - acc: 0.6700\n",
      "Epoch 186/500\n",
      "0s - loss: 0.0697 - acc: 0.6700\n",
      "Epoch 187/500\n",
      "0s - loss: 0.0696 - acc: 0.6700\n",
      "Epoch 188/500\n",
      "0s - loss: 0.0696 - acc: 0.6700\n",
      "Epoch 189/500\n",
      "0s - loss: 0.0696 - acc: 0.6700\n",
      "Epoch 190/500\n",
      "0s - loss: 0.0696 - acc: 0.6700\n",
      "Epoch 191/500\n",
      "0s - loss: 0.0696 - acc: 0.6700\n",
      "Epoch 192/500\n",
      "0s - loss: 0.0696 - acc: 0.6700\n",
      "Epoch 193/500\n",
      "0s - loss: 0.0696 - acc: 0.6700\n",
      "Epoch 194/500\n",
      "0s - loss: 0.0695 - acc: 0.6700\n",
      "Epoch 195/500\n",
      "0s - loss: 0.0695 - acc: 0.6700\n",
      "Epoch 196/500\n",
      "0s - loss: 0.0695 - acc: 0.6700\n",
      "Epoch 197/500\n",
      "0s - loss: 0.0695 - acc: 0.6700\n",
      "Epoch 198/500\n",
      "0s - loss: 0.0694 - acc: 0.6700\n",
      "Epoch 199/500\n",
      "0s - loss: 0.0695 - acc: 0.6700\n",
      "Epoch 200/500\n",
      "0s - loss: 0.0694 - acc: 0.6700\n",
      "Epoch 201/500\n",
      "0s - loss: 0.0694 - acc: 0.6700\n",
      "Epoch 202/500\n",
      "0s - loss: 0.0694 - acc: 0.6700\n",
      "Epoch 203/500\n",
      "0s - loss: 0.0693 - acc: 0.6700\n",
      "Epoch 204/500\n",
      "0s - loss: 0.0693 - acc: 0.6700\n",
      "Epoch 205/500\n",
      "0s - loss: 0.0693 - acc: 0.6700\n",
      "Epoch 206/500\n",
      "0s - loss: 0.0693 - acc: 0.6700\n",
      "Epoch 207/500\n",
      "0s - loss: 0.0694 - acc: 0.6700\n",
      "Epoch 208/500\n",
      "0s - loss: 0.0693 - acc: 0.6700\n",
      "Epoch 209/500\n",
      "0s - loss: 0.0692 - acc: 0.6700\n",
      "Epoch 210/500\n",
      "0s - loss: 0.0692 - acc: 0.6700\n",
      "Epoch 211/500\n",
      "0s - loss: 0.0692 - acc: 0.6700\n",
      "Epoch 212/500\n",
      "0s - loss: 0.0692 - acc: 0.6700\n",
      "Epoch 213/500\n",
      "0s - loss: 0.0692 - acc: 0.6700\n",
      "Epoch 214/500\n",
      "0s - loss: 0.0692 - acc: 0.6700\n",
      "Epoch 215/500\n",
      "0s - loss: 0.0691 - acc: 0.6700\n",
      "Epoch 216/500\n",
      "0s - loss: 0.0691 - acc: 0.6700\n",
      "Epoch 217/500\n",
      "0s - loss: 0.0691 - acc: 0.6700\n",
      "Epoch 218/500\n",
      "0s - loss: 0.0691 - acc: 0.6700\n",
      "Epoch 219/500\n",
      "0s - loss: 0.0691 - acc: 0.6700\n",
      "Epoch 220/500\n",
      "0s - loss: 0.0690 - acc: 0.6700\n",
      "Epoch 221/500\n",
      "0s - loss: 0.0690 - acc: 0.6700\n",
      "Epoch 222/500\n",
      "0s - loss: 0.0690 - acc: 0.6700\n",
      "Epoch 223/500\n",
      "0s - loss: 0.0691 - acc: 0.6700\n",
      "Epoch 224/500\n",
      "0s - loss: 0.0690 - acc: 0.6700\n",
      "Epoch 225/500\n",
      "0s - loss: 0.0690 - acc: 0.6700\n",
      "Epoch 226/500\n",
      "0s - loss: 0.0689 - acc: 0.6700\n",
      "Epoch 227/500\n",
      "0s - loss: 0.0689 - acc: 0.6700\n",
      "Epoch 228/500\n",
      "0s - loss: 0.0689 - acc: 0.6700\n",
      "Epoch 229/500\n",
      "0s - loss: 0.0689 - acc: 0.6700\n",
      "Epoch 230/500\n",
      "0s - loss: 0.0689 - acc: 0.6700\n",
      "Epoch 231/500\n",
      "0s - loss: 0.0688 - acc: 0.6700\n",
      "Epoch 232/500\n",
      "0s - loss: 0.0688 - acc: 0.6700\n",
      "Epoch 233/500\n",
      "0s - loss: 0.0688 - acc: 0.6700\n",
      "Epoch 234/500\n",
      "0s - loss: 0.0688 - acc: 0.6700\n",
      "Epoch 235/500\n",
      "0s - loss: 0.0688 - acc: 0.6700\n",
      "Epoch 236/500\n",
      "0s - loss: 0.0687 - acc: 0.6700\n",
      "Epoch 237/500\n",
      "0s - loss: 0.0687 - acc: 0.6700\n",
      "Epoch 238/500\n",
      "0s - loss: 0.0687 - acc: 0.6700\n",
      "Epoch 239/500\n",
      "0s - loss: 0.0687 - acc: 0.6700\n",
      "Epoch 240/500\n",
      "0s - loss: 0.0687 - acc: 0.6700\n",
      "Epoch 241/500\n",
      "0s - loss: 0.0687 - acc: 0.6700\n",
      "Epoch 242/500\n",
      "0s - loss: 0.0687 - acc: 0.6700\n",
      "Epoch 243/500\n",
      "0s - loss: 0.0686 - acc: 0.6700\n",
      "Epoch 244/500\n",
      "0s - loss: 0.0687 - acc: 0.6700\n",
      "Epoch 245/500\n",
      "0s - loss: 0.0686 - acc: 0.6700\n",
      "Epoch 246/500\n",
      "0s - loss: 0.0686 - acc: 0.6700\n",
      "Epoch 247/500\n",
      "0s - loss: 0.0686 - acc: 0.6700\n",
      "Epoch 248/500\n",
      "0s - loss: 0.0685 - acc: 0.6700\n",
      "Epoch 249/500\n",
      "0s - loss: 0.0685 - acc: 0.6700\n",
      "Epoch 250/500\n",
      "0s - loss: 0.0686 - acc: 0.6700\n",
      "Epoch 251/500\n",
      "0s - loss: 0.0685 - acc: 0.6700\n",
      "Epoch 252/500\n",
      "0s - loss: 0.0685 - acc: 0.6700\n",
      "Epoch 253/500\n",
      "0s - loss: 0.0684 - acc: 0.6700\n",
      "Epoch 254/500\n",
      "0s - loss: 0.0684 - acc: 0.6700\n",
      "Epoch 255/500\n",
      "0s - loss: 0.0684 - acc: 0.6700\n",
      "Epoch 256/500\n",
      "0s - loss: 0.0684 - acc: 0.6700\n",
      "Epoch 257/500\n",
      "0s - loss: 0.0684 - acc: 0.6700\n",
      "Epoch 258/500\n",
      "0s - loss: 0.0684 - acc: 0.6700\n",
      "Epoch 259/500\n",
      "0s - loss: 0.0683 - acc: 0.6700\n",
      "Epoch 260/500\n",
      "0s - loss: 0.0683 - acc: 0.6700\n",
      "Epoch 261/500\n",
      "0s - loss: 0.0683 - acc: 0.6700\n",
      "Epoch 262/500\n",
      "0s - loss: 0.0683 - acc: 0.6700\n",
      "Epoch 263/500\n",
      "0s - loss: 0.0682 - acc: 0.6700\n",
      "Epoch 264/500\n",
      "0s - loss: 0.0683 - acc: 0.6700\n",
      "Epoch 265/500\n",
      "0s - loss: 0.0682 - acc: 0.6700\n",
      "Epoch 266/500\n",
      "0s - loss: 0.0682 - acc: 0.6700\n",
      "Epoch 267/500\n",
      "0s - loss: 0.0682 - acc: 0.6700\n",
      "Epoch 268/500\n",
      "0s - loss: 0.0682 - acc: 0.6700\n",
      "Epoch 269/500\n",
      "0s - loss: 0.0682 - acc: 0.6700\n",
      "Epoch 270/500\n",
      "0s - loss: 0.0681 - acc: 0.6700\n",
      "Epoch 271/500\n",
      "0s - loss: 0.0681 - acc: 0.6700\n",
      "Epoch 272/500\n",
      "0s - loss: 0.0681 - acc: 0.6700\n",
      "Epoch 273/500\n",
      "0s - loss: 0.0681 - acc: 0.6700\n",
      "Epoch 274/500\n",
      "0s - loss: 0.0681 - acc: 0.6700\n",
      "Epoch 275/500\n",
      "0s - loss: 0.0681 - acc: 0.6700\n",
      "Epoch 276/500\n",
      "0s - loss: 0.0680 - acc: 0.6700\n",
      "Epoch 277/500\n",
      "0s - loss: 0.0680 - acc: 0.6700\n",
      "Epoch 278/500\n",
      "0s - loss: 0.0680 - acc: 0.6700\n",
      "Epoch 279/500\n",
      "0s - loss: 0.0680 - acc: 0.6700\n",
      "Epoch 280/500\n",
      "0s - loss: 0.0680 - acc: 0.6700\n",
      "Epoch 281/500\n",
      "0s - loss: 0.0680 - acc: 0.6700\n",
      "Epoch 282/500\n",
      "0s - loss: 0.0680 - acc: 0.6700\n",
      "Epoch 283/500\n",
      "0s - loss: 0.0679 - acc: 0.6700\n",
      "Epoch 284/500\n",
      "0s - loss: 0.0679 - acc: 0.6700\n",
      "Epoch 285/500\n",
      "0s - loss: 0.0679 - acc: 0.6700\n",
      "Epoch 286/500\n",
      "0s - loss: 0.0680 - acc: 0.6700\n",
      "Epoch 287/500\n",
      "0s - loss: 0.0679 - acc: 0.6700\n",
      "Epoch 288/500\n",
      "0s - loss: 0.0678 - acc: 0.6700\n",
      "Epoch 289/500\n",
      "0s - loss: 0.0678 - acc: 0.6700\n",
      "Epoch 290/500\n",
      "0s - loss: 0.0678 - acc: 0.6700\n",
      "Epoch 291/500\n",
      "0s - loss: 0.0678 - acc: 0.6700\n",
      "Epoch 292/500\n",
      "0s - loss: 0.0678 - acc: 0.6700\n",
      "Epoch 293/500\n",
      "0s - loss: 0.0678 - acc: 0.6700\n",
      "Epoch 294/500\n",
      "0s - loss: 0.0677 - acc: 0.6700\n",
      "Epoch 295/500\n",
      "0s - loss: 0.0678 - acc: 0.6700\n",
      "Epoch 296/500\n",
      "0s - loss: 0.0677 - acc: 0.6700\n",
      "Epoch 297/500\n",
      "0s - loss: 0.0677 - acc: 0.6700\n",
      "Epoch 298/500\n",
      "0s - loss: 0.0677 - acc: 0.6700\n",
      "Epoch 299/500\n",
      "0s - loss: 0.0677 - acc: 0.6700\n",
      "Epoch 300/500\n",
      "0s - loss: 0.0677 - acc: 0.6700\n",
      "Epoch 301/500\n",
      "0s - loss: 0.0676 - acc: 0.6700\n",
      "Epoch 302/500\n",
      "0s - loss: 0.0676 - acc: 0.6700\n",
      "Epoch 303/500\n",
      "0s - loss: 0.0676 - acc: 0.6700\n",
      "Epoch 304/500\n",
      "0s - loss: 0.0676 - acc: 0.6700\n",
      "Epoch 305/500\n",
      "0s - loss: 0.0676 - acc: 0.6700\n",
      "Epoch 306/500\n",
      "0s - loss: 0.0676 - acc: 0.6700\n",
      "Epoch 307/500\n",
      "0s - loss: 0.0676 - acc: 0.6700\n",
      "Epoch 308/500\n",
      "0s - loss: 0.0675 - acc: 0.6700\n",
      "Epoch 309/500\n",
      "0s - loss: 0.0675 - acc: 0.6700\n",
      "Epoch 310/500\n",
      "0s - loss: 0.0675 - acc: 0.6700\n",
      "Epoch 311/500\n",
      "0s - loss: 0.0675 - acc: 0.6700\n",
      "Epoch 312/500\n",
      "0s - loss: 0.0675 - acc: 0.6700\n",
      "Epoch 313/500\n",
      "0s - loss: 0.0674 - acc: 0.6700\n",
      "Epoch 314/500\n",
      "0s - loss: 0.0674 - acc: 0.6700\n",
      "Epoch 315/500\n",
      "0s - loss: 0.0674 - acc: 0.6700\n",
      "Epoch 316/500\n",
      "0s - loss: 0.0674 - acc: 0.6700\n",
      "Epoch 317/500\n",
      "0s - loss: 0.0674 - acc: 0.6700\n",
      "Epoch 318/500\n",
      "0s - loss: 0.0673 - acc: 0.6700\n",
      "Epoch 319/500\n",
      "0s - loss: 0.0673 - acc: 0.6700\n",
      "Epoch 320/500\n",
      "0s - loss: 0.0673 - acc: 0.6700\n",
      "Epoch 321/500\n",
      "0s - loss: 0.0673 - acc: 0.6700\n",
      "Epoch 322/500\n",
      "0s - loss: 0.0673 - acc: 0.6700\n",
      "Epoch 323/500\n",
      "0s - loss: 0.0673 - acc: 0.6700\n",
      "Epoch 324/500\n",
      "0s - loss: 0.0672 - acc: 0.6700\n",
      "Epoch 325/500\n",
      "0s - loss: 0.0672 - acc: 0.6700\n",
      "Epoch 326/500\n",
      "0s - loss: 0.0672 - acc: 0.6700\n",
      "Epoch 327/500\n",
      "0s - loss: 0.0672 - acc: 0.6700\n",
      "Epoch 328/500\n",
      "0s - loss: 0.0672 - acc: 0.6700\n",
      "Epoch 329/500\n",
      "0s - loss: 0.0672 - acc: 0.6700\n",
      "Epoch 330/500\n",
      "0s - loss: 0.0671 - acc: 0.6700\n",
      "Epoch 331/500\n",
      "0s - loss: 0.0671 - acc: 0.6700\n",
      "Epoch 332/500\n",
      "0s - loss: 0.0671 - acc: 0.6700\n",
      "Epoch 333/500\n",
      "0s - loss: 0.0671 - acc: 0.6700\n",
      "Epoch 334/500\n",
      "0s - loss: 0.0670 - acc: 0.6700\n",
      "Epoch 335/500\n",
      "0s - loss: 0.0671 - acc: 0.6700\n",
      "Epoch 336/500\n",
      "0s - loss: 0.0671 - acc: 0.6700\n",
      "Epoch 337/500\n",
      "0s - loss: 0.0670 - acc: 0.6700\n",
      "Epoch 338/500\n",
      "0s - loss: 0.0670 - acc: 0.6700\n",
      "Epoch 339/500\n",
      "0s - loss: 0.0670 - acc: 0.6700\n",
      "Epoch 340/500\n",
      "0s - loss: 0.0670 - acc: 0.6700\n",
      "Epoch 341/500\n",
      "0s - loss: 0.0670 - acc: 0.6700\n",
      "Epoch 342/500\n",
      "0s - loss: 0.0669 - acc: 0.6700\n",
      "Epoch 343/500\n",
      "0s - loss: 0.0669 - acc: 0.6700\n",
      "Epoch 344/500\n",
      "0s - loss: 0.0669 - acc: 0.6700\n",
      "Epoch 345/500\n",
      "0s - loss: 0.0669 - acc: 0.6700\n",
      "Epoch 346/500\n",
      "0s - loss: 0.0669 - acc: 0.6700\n",
      "Epoch 347/500\n",
      "0s - loss: 0.0668 - acc: 0.6700\n",
      "Epoch 348/500\n",
      "0s - loss: 0.0668 - acc: 0.6700\n",
      "Epoch 349/500\n",
      "0s - loss: 0.0669 - acc: 0.6700\n",
      "Epoch 350/500\n",
      "0s - loss: 0.0668 - acc: 0.6700\n",
      "Epoch 351/500\n",
      "0s - loss: 0.0669 - acc: 0.6700\n",
      "Epoch 352/500\n",
      "0s - loss: 0.0667 - acc: 0.6700\n",
      "Epoch 353/500\n",
      "0s - loss: 0.0668 - acc: 0.6700\n",
      "Epoch 354/500\n",
      "0s - loss: 0.0668 - acc: 0.6700\n",
      "Epoch 355/500\n",
      "0s - loss: 0.0668 - acc: 0.6700\n",
      "Epoch 356/500\n",
      "0s - loss: 0.0667 - acc: 0.6700\n",
      "Epoch 357/500\n",
      "0s - loss: 0.0667 - acc: 0.6700\n",
      "Epoch 358/500\n",
      "0s - loss: 0.0667 - acc: 0.6700\n",
      "Epoch 359/500\n",
      "0s - loss: 0.0667 - acc: 0.6700\n",
      "Epoch 360/500\n",
      "0s - loss: 0.0666 - acc: 0.6700\n",
      "Epoch 361/500\n",
      "0s - loss: 0.0667 - acc: 0.6700\n",
      "Epoch 362/500\n",
      "0s - loss: 0.0666 - acc: 0.6700\n",
      "Epoch 363/500\n",
      "0s - loss: 0.0667 - acc: 0.6700\n",
      "Epoch 364/500\n",
      "0s - loss: 0.0666 - acc: 0.6700\n",
      "Epoch 365/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0s - loss: 0.0666 - acc: 0.6700\n",
      "Epoch 366/500\n",
      "0s - loss: 0.0666 - acc: 0.6700\n",
      "Epoch 367/500\n",
      "0s - loss: 0.0665 - acc: 0.6700\n",
      "Epoch 368/500\n",
      "0s - loss: 0.0665 - acc: 0.6700\n",
      "Epoch 369/500\n",
      "0s - loss: 0.0665 - acc: 0.6700\n",
      "Epoch 370/500\n",
      "0s - loss: 0.0666 - acc: 0.6700\n",
      "Epoch 371/500\n",
      "0s - loss: 0.0664 - acc: 0.6700\n",
      "Epoch 372/500\n",
      "0s - loss: 0.0664 - acc: 0.6700\n",
      "Epoch 373/500\n",
      "0s - loss: 0.0664 - acc: 0.6700\n",
      "Epoch 374/500\n",
      "0s - loss: 0.0664 - acc: 0.6700\n",
      "Epoch 375/500\n",
      "0s - loss: 0.0664 - acc: 0.6700\n",
      "Epoch 376/500\n",
      "0s - loss: 0.0664 - acc: 0.6700\n",
      "Epoch 377/500\n",
      "0s - loss: 0.0663 - acc: 0.6700\n",
      "Epoch 378/500\n",
      "0s - loss: 0.0664 - acc: 0.6700\n",
      "Epoch 379/500\n",
      "0s - loss: 0.0663 - acc: 0.6700\n",
      "Epoch 380/500\n",
      "0s - loss: 0.0663 - acc: 0.6700\n",
      "Epoch 381/500\n",
      "0s - loss: 0.0665 - acc: 0.6700\n",
      "Epoch 382/500\n",
      "0s - loss: 0.0664 - acc: 0.6700\n",
      "Epoch 383/500\n",
      "0s - loss: 0.0663 - acc: 0.6700\n",
      "Epoch 384/500\n",
      "0s - loss: 0.0662 - acc: 0.6700\n",
      "Epoch 385/500\n",
      "0s - loss: 0.0662 - acc: 0.6700\n",
      "Epoch 386/500\n",
      "0s - loss: 0.0662 - acc: 0.6700\n",
      "Epoch 387/500\n",
      "0s - loss: 0.0662 - acc: 0.6700\n",
      "Epoch 388/500\n",
      "0s - loss: 0.0662 - acc: 0.6700\n",
      "Epoch 389/500\n",
      "0s - loss: 0.0662 - acc: 0.6700\n",
      "Epoch 390/500\n",
      "0s - loss: 0.0661 - acc: 0.6700\n",
      "Epoch 391/500\n",
      "0s - loss: 0.0662 - acc: 0.6700\n",
      "Epoch 392/500\n",
      "0s - loss: 0.0661 - acc: 0.6700\n",
      "Epoch 393/500\n",
      "0s - loss: 0.0661 - acc: 0.6700\n",
      "Epoch 394/500\n",
      "0s - loss: 0.0661 - acc: 0.6700\n",
      "Epoch 395/500\n",
      "0s - loss: 0.0661 - acc: 0.6700\n",
      "Epoch 396/500\n",
      "0s - loss: 0.0661 - acc: 0.6700\n",
      "Epoch 397/500\n",
      "0s - loss: 0.0660 - acc: 0.6700\n",
      "Epoch 398/500\n",
      "0s - loss: 0.0660 - acc: 0.6700\n",
      "Epoch 399/500\n",
      "0s - loss: 0.0660 - acc: 0.6700\n",
      "Epoch 400/500\n",
      "0s - loss: 0.0660 - acc: 0.6700\n",
      "Epoch 401/500\n",
      "0s - loss: 0.0660 - acc: 0.6700\n",
      "Epoch 402/500\n",
      "0s - loss: 0.0659 - acc: 0.6700\n",
      "Epoch 403/500\n",
      "0s - loss: 0.0659 - acc: 0.6700\n",
      "Epoch 404/500\n",
      "0s - loss: 0.0660 - acc: 0.6700\n",
      "Epoch 405/500\n",
      "0s - loss: 0.0659 - acc: 0.6700\n",
      "Epoch 406/500\n",
      "0s - loss: 0.0659 - acc: 0.6700\n",
      "Epoch 407/500\n",
      "0s - loss: 0.0659 - acc: 0.6700\n",
      "Epoch 408/500\n",
      "0s - loss: 0.0659 - acc: 0.6700\n",
      "Epoch 409/500\n",
      "0s - loss: 0.0658 - acc: 0.6700\n",
      "Epoch 410/500\n",
      "0s - loss: 0.0659 - acc: 0.6700\n",
      "Epoch 411/500\n",
      "0s - loss: 0.0659 - acc: 0.6700\n",
      "Epoch 412/500\n",
      "0s - loss: 0.0658 - acc: 0.6700\n",
      "Epoch 413/500\n",
      "0s - loss: 0.0658 - acc: 0.6700\n",
      "Epoch 414/500\n",
      "0s - loss: 0.0658 - acc: 0.6700\n",
      "Epoch 415/500\n",
      "0s - loss: 0.0657 - acc: 0.6700\n",
      "Epoch 416/500\n",
      "0s - loss: 0.0658 - acc: 0.6700\n",
      "Epoch 417/500\n",
      "0s - loss: 0.0657 - acc: 0.6700\n",
      "Epoch 418/500\n",
      "0s - loss: 0.0657 - acc: 0.6700\n",
      "Epoch 419/500\n",
      "0s - loss: 0.0659 - acc: 0.6700\n",
      "Epoch 420/500\n",
      "0s - loss: 0.0657 - acc: 0.6700\n",
      "Epoch 421/500\n",
      "0s - loss: 0.0656 - acc: 0.6700\n",
      "Epoch 422/500\n",
      "0s - loss: 0.0657 - acc: 0.6700\n",
      "Epoch 423/500\n",
      "0s - loss: 0.0656 - acc: 0.6700\n",
      "Epoch 424/500\n",
      "0s - loss: 0.0656 - acc: 0.6700\n",
      "Epoch 425/500\n",
      "0s - loss: 0.0656 - acc: 0.6700\n",
      "Epoch 426/500\n",
      "0s - loss: 0.0656 - acc: 0.6700\n",
      "Epoch 427/500\n",
      "0s - loss: 0.0655 - acc: 0.6700\n",
      "Epoch 428/500\n",
      "0s - loss: 0.0657 - acc: 0.6700\n",
      "Epoch 429/500\n",
      "0s - loss: 0.0655 - acc: 0.6700\n",
      "Epoch 430/500\n",
      "0s - loss: 0.0655 - acc: 0.6700\n",
      "Epoch 431/500\n",
      "0s - loss: 0.0655 - acc: 0.6700\n",
      "Epoch 432/500\n",
      "0s - loss: 0.0655 - acc: 0.6700\n",
      "Epoch 433/500\n",
      "0s - loss: 0.0654 - acc: 0.6700\n",
      "Epoch 434/500\n",
      "0s - loss: 0.0655 - acc: 0.6700\n",
      "Epoch 435/500\n",
      "0s - loss: 0.0655 - acc: 0.6700\n",
      "Epoch 436/500\n",
      "0s - loss: 0.0654 - acc: 0.6700\n",
      "Epoch 437/500\n",
      "0s - loss: 0.0654 - acc: 0.6700\n",
      "Epoch 438/500\n",
      "0s - loss: 0.0654 - acc: 0.6700\n",
      "Epoch 439/500\n",
      "0s - loss: 0.0654 - acc: 0.6700\n",
      "Epoch 440/500\n",
      "0s - loss: 0.0653 - acc: 0.6700\n",
      "Epoch 441/500\n",
      "0s - loss: 0.0654 - acc: 0.6700\n",
      "Epoch 442/500\n",
      "0s - loss: 0.0653 - acc: 0.6700\n",
      "Epoch 443/500\n",
      "0s - loss: 0.0654 - acc: 0.6700\n",
      "Epoch 444/500\n",
      "0s - loss: 0.0653 - acc: 0.6700\n",
      "Epoch 445/500\n",
      "0s - loss: 0.0653 - acc: 0.6700\n",
      "Epoch 446/500\n",
      "0s - loss: 0.0654 - acc: 0.6700\n",
      "Epoch 447/500\n",
      "0s - loss: 0.0653 - acc: 0.6700\n",
      "Epoch 448/500\n",
      "0s - loss: 0.0653 - acc: 0.6700\n",
      "Epoch 449/500\n",
      "0s - loss: 0.0653 - acc: 0.6700\n",
      "Epoch 450/500\n",
      "0s - loss: 0.0652 - acc: 0.6700\n",
      "Epoch 451/500\n",
      "0s - loss: 0.0652 - acc: 0.6700\n",
      "Epoch 452/500\n",
      "0s - loss: 0.0652 - acc: 0.6700\n",
      "Epoch 453/500\n",
      "0s - loss: 0.0652 - acc: 0.6700\n",
      "Epoch 454/500\n",
      "0s - loss: 0.0651 - acc: 0.6700\n",
      "Epoch 455/500\n",
      "0s - loss: 0.0652 - acc: 0.6700\n",
      "Epoch 456/500\n",
      "0s - loss: 0.0651 - acc: 0.6700\n",
      "Epoch 457/500\n",
      "0s - loss: 0.0651 - acc: 0.6700\n",
      "Epoch 458/500\n",
      "0s - loss: 0.0651 - acc: 0.6700\n",
      "Epoch 459/500\n",
      "0s - loss: 0.0652 - acc: 0.6700\n",
      "Epoch 460/500\n",
      "0s - loss: 0.0651 - acc: 0.6700\n",
      "Epoch 461/500\n",
      "0s - loss: 0.0650 - acc: 0.6700\n",
      "Epoch 462/500\n",
      "0s - loss: 0.0650 - acc: 0.6700\n",
      "Epoch 463/500\n",
      "0s - loss: 0.0650 - acc: 0.6700\n",
      "Epoch 464/500\n",
      "0s - loss: 0.0650 - acc: 0.6700\n",
      "Epoch 465/500\n",
      "0s - loss: 0.0650 - acc: 0.6700\n",
      "Epoch 466/500\n",
      "0s - loss: 0.0650 - acc: 0.6700\n",
      "Epoch 467/500\n",
      "0s - loss: 0.0649 - acc: 0.6700\n",
      "Epoch 468/500\n",
      "0s - loss: 0.0649 - acc: 0.6700\n",
      "Epoch 469/500\n",
      "0s - loss: 0.0649 - acc: 0.6700\n",
      "Epoch 470/500\n",
      "0s - loss: 0.0650 - acc: 0.6700\n",
      "Epoch 471/500\n",
      "0s - loss: 0.0648 - acc: 0.6700\n",
      "Epoch 472/500\n",
      "0s - loss: 0.0649 - acc: 0.6700\n",
      "Epoch 473/500\n",
      "0s - loss: 0.0649 - acc: 0.6700\n",
      "Epoch 474/500\n",
      "0s - loss: 0.0648 - acc: 0.6700\n",
      "Epoch 475/500\n",
      "0s - loss: 0.0649 - acc: 0.6700\n",
      "Epoch 476/500\n",
      "0s - loss: 0.0648 - acc: 0.6700\n",
      "Epoch 477/500\n",
      "0s - loss: 0.0649 - acc: 0.6700\n",
      "Epoch 478/500\n",
      "0s - loss: 0.0648 - acc: 0.6700\n",
      "Epoch 479/500\n",
      "0s - loss: 0.0648 - acc: 0.6700\n",
      "Epoch 480/500\n",
      "0s - loss: 0.0648 - acc: 0.6700\n",
      "Epoch 481/500\n",
      "0s - loss: 0.0648 - acc: 0.6700\n",
      "Epoch 482/500\n",
      "0s - loss: 0.0648 - acc: 0.6700\n",
      "Epoch 483/500\n",
      "0s - loss: 0.0648 - acc: 0.6700\n",
      "Epoch 484/500\n",
      "0s - loss: 0.0647 - acc: 0.6700\n",
      "Epoch 485/500\n",
      "0s - loss: 0.0647 - acc: 0.6700\n",
      "Epoch 486/500\n",
      "0s - loss: 0.0646 - acc: 0.6700\n",
      "Epoch 487/500\n",
      "0s - loss: 0.0646 - acc: 0.6700\n",
      "Epoch 488/500\n",
      "0s - loss: 0.0646 - acc: 0.6700\n",
      "Epoch 489/500\n",
      "0s - loss: 0.0647 - acc: 0.6700\n",
      "Epoch 490/500\n",
      "0s - loss: 0.0647 - acc: 0.6700\n",
      "Epoch 491/500\n",
      "0s - loss: 0.0646 - acc: 0.6700\n",
      "Epoch 492/500\n",
      "0s - loss: 0.0646 - acc: 0.6700\n",
      "Epoch 493/500\n",
      "0s - loss: 0.0646 - acc: 0.6700\n",
      "Epoch 494/500\n",
      "0s - loss: 0.0647 - acc: 0.6700\n",
      "Epoch 495/500\n",
      "0s - loss: 0.0645 - acc: 0.6700\n",
      "Epoch 496/500\n",
      "0s - loss: 0.0645 - acc: 0.6700\n",
      "Epoch 497/500\n",
      "0s - loss: 0.0645 - acc: 0.6700\n",
      "Epoch 498/500\n",
      "0s - loss: 0.0645 - acc: 0.6700\n",
      "Epoch 499/500\n",
      "0s - loss: 0.0645 - acc: 0.6700\n",
      "Epoch 500/500\n",
      "0s - loss: 0.0644 - acc: 0.6700\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f17a5f47860>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, epochs=500, batch_size=10,  verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.46284794e-02, 1.38777878e-16])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.38057196],\n",
       "       [ 0.3676772 ],\n",
       "       [ 0.36016142],\n",
       "       [ 0.35363817],\n",
       "       [-0.3363304 ]], dtype=float32)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(x_test[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.38057196],\n",
       "       [ 0.3676772 ],\n",
       "       [ 0.36016142],\n",
       "       [ 0.35363817],\n",
       "       [-0.3363304 ],\n",
       "       [ 0.37701893],\n",
       "       [-0.02322316],\n",
       "       [ 0.7715844 ],\n",
       "       [ 1.2275703 ],\n",
       "       [-0.6968577 ],\n",
       "       [ 1.8844777 ],\n",
       "       [ 1.2487319 ],\n",
       "       [ 1.2571735 ],\n",
       "       [ 1.2431394 ],\n",
       "       [ 1.8829366 ],\n",
       "       [ 1.2610192 ],\n",
       "       [ 1.2499195 ],\n",
       "       [ 1.2568063 ],\n",
       "       [ 1.8873212 ],\n",
       "       [ 1.8842999 ]], dtype=float32)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
